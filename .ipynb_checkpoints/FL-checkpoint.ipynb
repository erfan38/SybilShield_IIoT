{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "447f7d67-5ad4-481f-bcc6-ee91fe9800dc",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9baef8-ff4a-4659-8ea2-6863e0ccbbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import os, json\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f370bd2-fd99-4b0a-9c5d-d15e35230b9b",
   "metadata": {},
   "source": [
    "## Step0: Simulator ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672ea2a9-40a8-40c8-8dd1-ac19183e1be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------- Configuration ----------------------\n",
    "PROTOCOLS = [\n",
    "    \"MQTT:PUBLISH\", \"MQTT:SUBSCRIBE\", \"MQTT:UNSUBSCRIBE\",\n",
    "    \"libp2p:GRAFT\", \"libp2p:PRUNE\", \"libp2p:IHAVE\", \"libp2p:IWANT\",\n",
    "    \"devp2p:GetBlockHeaders\", \"devp2p:Transactions\", \"devp2p:NewBlock\",\n",
    "    \"discovery:Ping\", \"discovery:Pong\", \"discovery:FindNode\", \"discovery:Neighbors\",\n",
    "    \"ENR:Request\", \"ENR:Response\"\n",
    "]\n",
    "\n",
    "LOG_FILE = \"network_logs.jsonl\"\n",
    "\n",
    "# ---------------------- Node Class ----------------------\n",
    "class Node:\n",
    "    def __init__(self, node_id, is_sybil=False):\n",
    "        self.node_id = node_id\n",
    "        self.is_sybil = is_sybil\n",
    "        self.behavior_type = random.choice([\"spammer\", \"evasive\", \"flooder\"]) if is_sybil else \"normal\"\n",
    "\n",
    "    def send_message(self, network, forced_target=None):\n",
    "        target = forced_target or network.random_node(exclude=self.node_id)\n",
    "        if not target:\n",
    "            return\n",
    "\n",
    "        if self.is_sybil:\n",
    "            if self.behavior_type == \"spammer\":\n",
    "                protocol = random.choice([\"MQTT:PUBLISH\", \"MQTT:PUBLISH\", \"MQTT:PUBLISH\"])\n",
    "            elif self.behavior_type == \"evasive\":\n",
    "                protocol = random.choice([\"ENR:Request\", \"libp2p:PRUNE\"])\n",
    "            else:  # flooder\n",
    "                protocol = random.choice([\"discovery:Ping\", \"discovery:FindNode\"])\n",
    "        else:\n",
    "            protocol = random.choice(PROTOCOLS)\n",
    "\n",
    "        msg = {\n",
    "            \"from\": self.node_id,\n",
    "            \"to\": target.node_id,\n",
    "            \"protocol\": protocol,\n",
    "            \"timestamp\": time.time(),\n",
    "            \"is_sybil\": self.is_sybil\n",
    "        }\n",
    "        network.logs.append(msg)\n",
    "\n",
    "# ---------------------- Network Class ----------------------\n",
    "class Network:\n",
    "    def __init__(self, honest_count=50, sybil_count=10):\n",
    "        self.nodes = {}\n",
    "        self.logs = []\n",
    "        for i in range(honest_count):\n",
    "            self.nodes[f\"Node{i:03}\"] = Node(f\"Node{i:03}\", is_sybil=False)\n",
    "        for i in range(sybil_count):\n",
    "            self.nodes[f\"Sybil{i:03}\"] = Node(f\"Sybil{i:03}\", is_sybil=True)\n",
    "\n",
    "    def random_node(self, exclude=None, honest_only=False, sybil_only=False):\n",
    "        candidates = [\n",
    "            n for n in self.nodes.values()\n",
    "            if n.node_id != exclude and\n",
    "            (not honest_only or not n.is_sybil) and\n",
    "            (not sybil_only or n.is_sybil)\n",
    "        ]\n",
    "        return random.choice(candidates) if candidates else None\n",
    "\n",
    "    def simulate_round(self, messages_per_node=10):\n",
    "        for node in self.nodes.values():\n",
    "            for _ in range(messages_per_node):\n",
    "                node.send_message(self)\n",
    "\n",
    "    def inject_sybil_to_honest(self, count=3):\n",
    "        honest_nodes = [n for n in self.nodes.values() if not n.is_sybil]\n",
    "        sybil_nodes = [n for n in self.nodes.values() if n.is_sybil]\n",
    "        for h in honest_nodes:\n",
    "            selected_sybils = random.sample(sybil_nodes, k=min(count, len(sybil_nodes)))\n",
    "            for s in selected_sybils:\n",
    "                s.send_message(self, forced_target=h)\n",
    "\n",
    "    def export_logs(self):\n",
    "        with open(LOG_FILE, \"w\") as f:\n",
    "            for m in self.logs:\n",
    "                f.write(json.dumps(m) + \"\\n\")\n",
    "        print(f\"[✓] Exported {len(self.logs)} messages to {LOG_FILE}\")\n",
    "\n",
    "# ---------------------- Run Simulation ----------------------\n",
    "if __name__ == \"__main__\":\n",
    "    sim = Network()\n",
    "    for round_num in range(5):\n",
    "        print(f\"[+] Simulating Round {round_num + 1}\")\n",
    "        sim.simulate_round(messages_per_node=10)\n",
    "        sim.inject_sybil_to_honest(count=2)\n",
    "        time.sleep(0.05)\n",
    "    sim.export_logs()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30248f84-0bbf-433e-a1d0-8356ec4a7026",
   "metadata": {},
   "source": [
    "## Step1: Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5090c8e8-68c8-47dc-98a6-48822c9bbb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load and sort logs\n",
    "with open(\"network_logs.jsonl\", \"r\") as f:\n",
    "    logs = [json.loads(line) for line in f]\n",
    "\n",
    "logs.sort(key=lambda x: x[\"timestamp\"])\n",
    "\n",
    "WINDOW_SIZE = 5\n",
    "node_messages = defaultdict(list)\n",
    "\n",
    "# Organize messages per node\n",
    "for log in logs:\n",
    "    node_messages[log[\"from\"]].append((\"sent\", log))\n",
    "    node_messages[log[\"to\"]].append((\"received\", log))\n",
    "\n",
    "rows = []\n",
    "\n",
    "# Generate features per node in time windows\n",
    "for node_id, messages in node_messages.items():\n",
    "    messages.sort(key=lambda x: x[1][\"timestamp\"])\n",
    "    for i in range(0, len(messages) - WINDOW_SIZE + 1, WINDOW_SIZE):\n",
    "        window = messages[i:i + WINDOW_SIZE]\n",
    "        sent_total = sent_to_sybil = sent_to_honest = 0\n",
    "        received_total = received_from_sybil = received_from_honest = 0\n",
    "        protocols = set()\n",
    "        timestamps = []\n",
    "\n",
    "        for direction, msg in window:\n",
    "            proto = msg[\"protocol\"]\n",
    "            timestamps.append(msg[\"timestamp\"])\n",
    "            protocols.add(proto)\n",
    "            if direction == \"sent\":\n",
    "                sent_total += 1\n",
    "                if msg[\"is_sybil\"]:\n",
    "                    sent_to_sybil += 1\n",
    "                else:\n",
    "                    sent_to_honest += 1\n",
    "            else:\n",
    "                received_total += 1\n",
    "                if msg[\"is_sybil\"]:\n",
    "                    received_from_sybil += 1\n",
    "                else:\n",
    "                    received_from_honest += 1\n",
    "\n",
    "        burstiness = 0.0\n",
    "        if len(timestamps) > 1:\n",
    "            gaps = np.diff(sorted(timestamps))\n",
    "            if len(gaps) > 1:\n",
    "                burstiness = np.std(gaps)\n",
    "\n",
    "        row = {\n",
    "            \"node_id\": node_id,\n",
    "            \"sent_total\": sent_total,\n",
    "            \"sent_to_sybil\": sent_to_sybil,\n",
    "            \"sent_to_honest\": sent_to_honest,\n",
    "            \"received_total\": received_total,\n",
    "            \"received_from_sybil\": received_from_sybil,\n",
    "            \"received_from_honest\": received_from_honest,\n",
    "            \"protocol_diversity\": len(protocols),\n",
    "            \"message_burstiness\": round(burstiness, 5),\n",
    "            \"is_sybil\": node_id.startswith(\"Sybil\")\n",
    "        }\n",
    "        rows.append(row)\n",
    "\n",
    "# Output to features_per_node/\n",
    "os.makedirs(\"features_per_node\", exist_ok=True)\n",
    "df_all = pd.DataFrame(rows)\n",
    "\n",
    "for node_id, node_df in df_all.groupby(\"node_id\"):\n",
    "    node_df.to_csv(f\"features_per_node/{node_id}.csv\", index=False)\n",
    "\n",
    "print(f\"[✓] Extracted {len(df_all)} samples across {len(df_all['node_id'].unique())} nodes into features_per_node/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741bcab0-d28d-4bdc-aba3-0889f45e6cf5",
   "metadata": {},
   "source": [
    "## Step2: Augmentor node data with Sybil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519f58aa-b76a-47dd-ad7a-6c8b6524a9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_DIR = \"features_per_node\"\n",
    "OUTPUT_DIR = \"data_per_node\"\n",
    "SYBIL_PER_HONEST = 5\n",
    "PERSONALITIES = [\"spammer\", \"evasive\", \"flooder\"]\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "for fname in sorted(os.listdir(FEATURES_DIR)):\n",
    "    if not fname.endswith(\".csv\") or fname.startswith(\"Sybil\"):\n",
    "        continue\n",
    "\n",
    "    node_id = fname.replace(\".csv\", \"\")\n",
    "    df = pd.read_csv(os.path.join(FEATURES_DIR, fname))\n",
    "\n",
    "    df[\"is_sybil\"] = False\n",
    "    honest_path = os.path.join(OUTPUT_DIR, f\"{node_id}.csv\")\n",
    "    df.to_csv(honest_path, index=False)\n",
    "    print(f\"[✓] Saved honest node: {node_id}.csv\")\n",
    "\n",
    "    # Create unique Sybil clones\n",
    "    for i in range(SYBIL_PER_HONEST):\n",
    "        sybil_id = f\"{node_id}_Sybil{str(i).zfill(2)}\"\n",
    "        sybil_df = df.copy()\n",
    "        sybil_df[\"node_id\"] = sybil_id\n",
    "        sybil_df[\"is_sybil\"] = True\n",
    "\n",
    "        role = random.choice(PERSONALITIES)\n",
    "\n",
    "        if role == \"spammer\":\n",
    "            sybil_df[\"sent_total\"] *= np.random.randint(2, 4)\n",
    "            sybil_df[\"protocol_diversity\"] = 1\n",
    "            sybil_df[\"message_burstiness\"] *= 1.2\n",
    "\n",
    "        elif role == \"evasive\":\n",
    "            sybil_df[\"sent_total\"] *= 0.5\n",
    "            sybil_df[\"received_total\"] *= 1.2\n",
    "            sybil_df[\"protocol_diversity\"] = np.random.randint(2, 4)\n",
    "            sybil_df[\"message_burstiness\"] *= np.random.uniform(2.0, 3.5)\n",
    "\n",
    "        elif role == \"flooder\":\n",
    "            sybil_df[\"sent_to_sybil\"] += np.random.randint(5, 10)\n",
    "            sybil_df[\"protocol_diversity\"] = 1\n",
    "            sybil_df[\"message_burstiness\"] *= 3\n",
    "            sybil_df[\"sent_total\"] *= np.random.randint(3, 6)\n",
    "\n",
    "        sybil_path = os.path.join(OUTPUT_DIR, f\"{sybil_id}.csv\")\n",
    "        sybil_df.to_csv(sybil_path, index=False)\n",
    "        print(f\"[✓] Created Sybil clone: {sybil_id} as {role}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79423ac-17bd-48c3-b512-3fe82296b93b",
   "metadata": {},
   "source": [
    "## Verifying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c56efa-a0bd-4ecb-aa31-5f2a3af50f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"data_per_node\"\n",
    "honest_count = 0\n",
    "sybil_count = 0\n",
    "errors = []\n",
    "\n",
    "for fname in sorted(os.listdir(DATA_DIR)):\n",
    "    if not fname.endswith(\".csv\"):\n",
    "        continue\n",
    "\n",
    "    path = os.path.join(DATA_DIR, fname)\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "\n",
    "        if \"is_sybil\" not in df.columns:\n",
    "            errors.append(f\"[✗] Missing 'is_sybil' column in {fname}\")\n",
    "            continue\n",
    "\n",
    "        if df[\"is_sybil\"].nunique() > 1:\n",
    "            errors.append(f\"[✗] Mixed labels in {fname}\")\n",
    "            continue\n",
    "\n",
    "        if df.isnull().values.any():\n",
    "            errors.append(f\"[✗] NaN values in {fname}\")\n",
    "\n",
    "        if (df.select_dtypes(include='number') < 0).any().any():\n",
    "            errors.append(f\"[✗] Negative values in numeric features in {fname}\")\n",
    "\n",
    "        if df[\"is_sybil\"].iloc[0]:\n",
    "            sybil_count += 1\n",
    "        else:\n",
    "            honest_count += 1\n",
    "\n",
    "    except Exception as e:\n",
    "        errors.append(f\"[✗] Failed to read {fname}: {str(e)}\")\n",
    "\n",
    "print(f\"\\n[✓] Total honest nodes: {honest_count}\")\n",
    "print(f\"[✓] Total Sybil clones: {sybil_count}\")\n",
    "print(f\"[✓] Verified {honest_count + sybil_count} files\")\n",
    "\n",
    "if errors:\n",
    "    print(\"\\n[⚠] Issues found:\")\n",
    "    for e in errors:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"\\n[✓] All files passed integrity checks.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a52a723-c68e-4005-8c1d-e31afdbfada2",
   "metadata": {},
   "source": [
    "## validation_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6311debd-509d-49f6-a7ac-a7257e259163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_validation_set.py\n",
    "\n",
    "DATA_DIR = \"data_per_node\"\n",
    "VALIDATION_SIZE = 300  # total samples\n",
    "\n",
    "all_dfs = []\n",
    "for file in os.listdir(DATA_DIR):\n",
    "    if file.endswith(\".csv\"):\n",
    "        df = pd.read_csv(os.path.join(DATA_DIR, file))\n",
    "        all_dfs.append(df)\n",
    "\n",
    "df_all = pd.concat(all_dfs, ignore_index=True)\n",
    "df_val = df_all.sample(n=VALIDATION_SIZE, random_state=42)\n",
    "df_val.to_csv(\"validation_set.csv\", index=False)\n",
    "print(f\"[✓] Saved validation set with {len(df_val)} samples to validation_set.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4bf257-bcb5-4451-89ce-6a094f197fd5",
   "metadata": {},
   "source": [
    "## Step5: Federated Learning round:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c7099f-f23a-402a-bc8c-bd680cfeee48",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "class TorchNodeModel(torch.nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(TorchNodeModel, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_size, 16)\n",
    "        self.fc2 = torch.nn.Linear(16, 8)\n",
    "        self.fc3 = torch.nn.Linear(8, 1)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.sigmoid(self.fc3(x))\n",
    "\n",
    "def evaluate_model_on_validation(model, val_df):\n",
    "    label_col = \"label\" if \"label\" in val_df.columns else \"is_sybil\"\n",
    "    X = torch.tensor(val_df.drop(columns=[label_col, \"node_id\"]).values, dtype=torch.float32)\n",
    "    y = torch.tensor(val_df[label_col].astype(float).values, dtype=torch.float32).unsqueeze(1)\n",
    "    with torch.no_grad():\n",
    "        pred = model(X)\n",
    "        pred_binary = (pred > 0.5).float()\n",
    "        accuracy = (pred_binary == y).float().mean().item()\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def train_node_model(df, input_size):\n",
    "    label_col = \"label\" if \"label\" in df.columns else \"is_sybil\"\n",
    "    X = torch.tensor(df.drop(columns=[label_col, \"node_id\"]).values, dtype=torch.float32)\n",
    "    y = torch.tensor(df[label_col].astype(float).values, dtype=torch.float32).unsqueeze(1)\n",
    "    model = TorchNodeModel(input_size)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    loss_fn = torch.nn.BCELoss()\n",
    "    for epoch in range(10):\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return model.state_dict()\n",
    "\n",
    "def average_models(model_dicts):\n",
    "    avg_model = {}\n",
    "    for key in model_dicts[0].keys():\n",
    "        avg_model[key] = sum(model[key] for model in model_dicts) / len(model_dicts)\n",
    "    return avg_model\n",
    "\n",
    "def load_node_data(data_dir):\n",
    "    data = []\n",
    "    for file in sorted(os.listdir(data_dir)):\n",
    "        if file.endswith(\".csv\"):\n",
    "            df = pd.read_csv(os.path.join(data_dir, file))\n",
    "            data.append((file, df))\n",
    "    return data\n",
    "\n",
    "def run_federated_rounds(data_dir=\"data_per_node\", rounds=5):\n",
    "    node_data = load_node_data(data_dir)\n",
    "    label_col = \"label\" if \"label\" in node_data[0][1].columns else \"is_sybil\"\n",
    "    input_size = node_data[0][1].drop(columns=[label_col, \"node_id\"]).shape[1]\n",
    "\n",
    "    global_model = TorchNodeModel(input_size)\n",
    "    accuracy_log = []\n",
    "\n",
    "    for rnd in range(rounds):\n",
    "        local_models = []\n",
    "        round_accuracies = []\n",
    "\n",
    "        for fname, df in node_data:\n",
    "            model_weights = train_node_model(df, input_size)\n",
    "            local_models.append(model_weights)\n",
    "\n",
    "            global_model.load_state_dict(model_weights)\n",
    "            acc = evaluate_model(global_model, df)\n",
    "            round_accuracies.append(acc)\n",
    "\n",
    "        avg_model = average_models(local_models)\n",
    "        global_model.load_state_dict(avg_model)\n",
    "        torch.save(global_model.state_dict(), f\"global_model_round{rnd+1}.pth\")\n",
    "\n",
    "        avg_acc = sum(round_accuracies) / len(round_accuracies)\n",
    "        accuracy_log.append(avg_acc)\n",
    "        print(f\"[Round {rnd+1}] Avg Accuracy: {avg_acc:.4f}\")\n",
    "\n",
    "    plt.plot(range(1, rounds + 1), accuracy_log, marker='o')\n",
    "    plt.xlabel(\"Round\")\n",
    "    plt.ylabel(\"Average Accuracy\")\n",
    "    plt.title(\"Federated Learning Accuracy over Rounds\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(\"fl_accuracy_trend.png\")\n",
    "    print(\"Saved accuracy trend plot as fl_accuracy_trend.png\")\n",
    "    # === Save node reputations based on global model ===\n",
    "    reputations = {}\n",
    "    val_df = pd.read_csv(\"validation_set.csv\")\n",
    "\n",
    "    for fname, df in node_data:\n",
    "        acc = evaluate_model_on_validation(global_model, val_df)\n",
    "        reputations[fname] = {\n",
    "            \"accuracy\": round(acc, 4),\n",
    "            \"reputation\": round(acc * 10 - 5, 3)\n",
    "        }\n",
    "\n",
    "    with open(\"node_reputations.json\", \"w\") as f:\n",
    "        json.dump(reputations, f, indent=2)\n",
    "    print(\" Exported node_reputations.json\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_federated_rounds()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42afbbb-e6b9-454d-84d2-0585774172a7",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b11069-f3fd-4424-af25-0175e29a4770",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"node_reputations.json\") as f:\n",
    "    reputations = json.load(f)\n",
    "y_true = []\n",
    "y_pred = []\n",
    "print(\"\\n[Node Reputations]\")\n",
    "print(\"-\" * 40)\n",
    "for node, info in sorted(reputations.items()):\n",
    "    rep = info[\"reputation\"]\n",
    "    acc = info[\"accuracy\"]\n",
    "    tag = \"Sybil?\" if \"Sybil\" in node or rep < 0 else \"✅ Honest\"\n",
    "    print(f\"{node:25}  Acc: {acc:.3f}  Rep: {rep:.2f}  {tag}\")\n",
    "\n",
    "for node_id, info in reputations.items():\n",
    "    true_label = 1 if \"Sybil\" in node_id else 0\n",
    "    predicted_label = 1 if info[\"reputation\"] < 0 else 0  # threshold\n",
    "\n",
    "    y_true.append(true_label)\n",
    "    y_pred.append(predicted_label)\n",
    "\n",
    "print(\"\\n[Classification Report based on reputation threshold 0]\")\n",
    "print(classification_report(y_true, y_pred, target_names=[\"Honest\", \"Sybil\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483f49d7-52cd-413b-83e0-57d06f2f9ea5",
   "metadata": {},
   "source": [
    "## Step6 Autoencoder anomaly detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b68b0b0-6b5b-49a5-85f2-79c9445a7e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 8), nn.ReLU(), nn.Linear(8, 3)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(3, 8), nn.ReLU(), nn.Linear(8, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        return self.decoder(z)\n",
    "\n",
    "def train_autoencoder(df, input_size):\n",
    "    X = torch.tensor(df.values, dtype=torch.float32)\n",
    "    model = AutoEncoder(input_size)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    for epoch in range(30):\n",
    "        optimizer.zero_grad()\n",
    "        out = model(X)\n",
    "        loss = loss_fn(out, X)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return model\n",
    "\n",
    "def detect_anomalies(model, df, threshold=0.1):\n",
    "    X = torch.tensor(df.values, dtype=torch.float32)\n",
    "    recon = model(X)\n",
    "    mse = ((X - recon)**2).mean(dim=1)\n",
    "    return (mse > threshold).nonzero(as_tuple=True)[0]\n",
    "\n",
    "data_dir = \"data_per_node\"\n",
    "for fname in sorted(os.listdir(data_dir)):\n",
    "    if fname.endswith(\".csv\"):\n",
    "        print(f\"\\nProcessing {fname}\")\n",
    "        df = pd.read_csv(os.path.join(data_dir, fname))\n",
    "        label_col = \"label\" if \"label\" in df.columns else \"is_sybil\"\n",
    "        X = df.drop(columns=[label_col, \"node_id\"])\n",
    "        model = train_autoencoder(X, input_size=X.shape[1])\n",
    "        anomalies = detect_anomalies(model, X)\n",
    "        print(f\"Anomalies in {fname}: {list(anomalies.numpy())}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b07cdd7-6a41-4438-9782-fdaec02f1103",
   "metadata": {},
   "source": [
    "## Step7: Torch Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106ac740-fb47-4b4e-a875-ddcd94b13d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchNodeModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(TorchNodeModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 16)\n",
    "        self.fc2 = nn.Linear(16, 8)\n",
    "        self.fc3 = nn.Linear(8, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.sigmoid(self.fc3(x))\n",
    "\n",
    "def train_node_model(path):\n",
    "    df = pd.read_csv(path)\n",
    "    label_col = \"label\" if \"label\" in df.columns else \"is_sybil\"\n",
    "    X = torch.tensor(df.drop(columns=[label_col, \"node_id\"]).values, dtype=torch.float32)\n",
    "    y = torch.tensor(df[label_col].astype(float).values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "    model = TorchNodeModel(X.shape[1])\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    for epoch in range(20):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X)\n",
    "        loss = criterion(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return model.state_dict()\n",
    "\n",
    "# Pick a valid node file to test\n",
    "test_file = \"data_per_node/Node000.csv\"\n",
    "model_weights = train_node_model(test_file)\n",
    "torch.save(model_weights, \"global_model.pth\")\n",
    "print(f\"[✓] Trained and saved model for {test_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a2714d-5b1e-493a-9f65-412457c1063e",
   "metadata": {},
   "source": [
    "## Step8: Initialize Smart contract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13457051-8b10-4cc2-a50a-6b6e25608756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# contract_simulation.py\n",
    "# Simulate smart contract logic in-memory\n",
    "\n",
    "class FederatedContractSimulator:\n",
    "    def __init__(self):\n",
    "        self.model_updates = {}\n",
    "\n",
    "    def submit_model_update(self, node_id, accuracy):\n",
    "        self.model_updates[node_id] = accuracy\n",
    "        print(f\"[+] {node_id} submitted update with accuracy {accuracy:.3f}\")\n",
    "\n",
    "    def aggregate(self):\n",
    "        if not self.model_updates:\n",
    "            return None\n",
    "        avg = sum(self.model_updates.values()) / len(self.model_updates)\n",
    "        print(f\"[*] Aggregated model accuracy: {avg:.3f}\")\n",
    "        return avg\n",
    "\n",
    "class ReputationContractSimulator:\n",
    "    def __init__(self, reputations):\n",
    "        self.reputation_scores = reputations\n",
    "\n",
    "    def flag_suspicious_nodes(self, threshold=-3.0):\n",
    "        flagged = [nid for nid, r in self.reputation_scores.items() if r['reputation'] <= threshold]\n",
    "        print(f\"[!] Flagged nodes for banning: {flagged}\")\n",
    "        return flagged\n",
    "\n",
    "# Load reputation scores\n",
    "with open(\"node_reputations.json\") as f:\n",
    "    reputations = json.load(f)\n",
    "\n",
    "# Simulate reputation smart contract\n",
    "rep_contract = ReputationContractSimulator(reputations)\n",
    "flagged_nodes = rep_contract.flag_suspicious_nodes(threshold=-3.5)\n",
    "\n",
    "# Simulate federated learning contract\n",
    "fl_contract = FederatedContractSimulator()\n",
    "for node_id in reputations:\n",
    "    acc = max(0.5, 1.0 + reputations[node_id]['reputation'] / 10.0)  # Fake accuracy\n",
    "    fl_contract.submit_model_update(node_id, acc)\n",
    "fl_contract.aggregate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087d4f45-57dd-42af-ba6d-efeeb0fb2546",
   "metadata": {},
   "source": [
    "## Comparison of Local VS Global fl training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4788dd66-0f95-4257-9330-7d7f8ab2751c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchNodeModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(TorchNodeModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 16)\n",
    "        self.fc2 = nn.Linear(16, 8)\n",
    "        self.fc3 = nn.Linear(8, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.sigmoid(self.fc3(x))\n",
    "\n",
    "def load_and_prepare_data(path):\n",
    "    df = pd.read_csv(path)\n",
    "    label_col = \"label\" if \"label\" in df.columns else \"is_sybil\"\n",
    "    X = torch.tensor(df.drop(columns=[label_col, \"node_id\"]).values, dtype=torch.float32)\n",
    "    y = torch.tensor(df[label_col].astype(float).values, dtype=torch.float32).unsqueeze(1)\n",
    "    return X, y, df[label_col].astype(int).values, df\n",
    "\n",
    "def predict_and_evaluate(model, X, y_true):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = (model(X) > 0.5).int().numpy()\n",
    "    labels = [0, 1]\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    report = classification_report(y_true, y_pred, labels=labels, output_dict=True)\n",
    "    return report, cm, y_pred\n",
    "\n",
    "# Automatically pick a Sybil node\n",
    "data_dir = \"data_per_node\"\n",
    "sybil_file = None\n",
    "\n",
    "for fname in os.listdir(data_dir):\n",
    "    if not fname.endswith(\".csv\"):\n",
    "        continue\n",
    "    df_check = pd.read_csv(os.path.join(data_dir, fname))\n",
    "    label_col = \"label\" if \"label\" in df_check.columns else \"is_sybil\"\n",
    "    if df_check[label_col].astype(int).sum() > 0:  # has at least one Sybil label\n",
    "        sybil_file = fname\n",
    "        break\n",
    "\n",
    "if sybil_file is None:\n",
    "    print(\"❌ No Sybil-labeled node found.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"[✓] Found Sybil-labeled node: {sybil_file}\")\n",
    "file_path = os.path.join(data_dir, sybil_file)\n",
    "X, y, y_true, df_meta = load_and_prepare_data(file_path)\n",
    "input_size = X.shape[1]\n",
    "\n",
    "# Load local model\n",
    "local_model = TorchNodeModel(input_size)\n",
    "local_model.load_state_dict(torch.load(\"global_model.pth\"))\n",
    "local_report, local_cm, _ = predict_and_evaluate(local_model, X, y_true)\n",
    "\n",
    "# Load global model\n",
    "global_model = TorchNodeModel(input_size)\n",
    "global_model.load_state_dict(torch.load(\"global_model_round5.pth\"))\n",
    "global_report, global_cm, _ = predict_and_evaluate(global_model, X, y_true)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "disp1 = ConfusionMatrixDisplay(confusion_matrix=local_cm, display_labels=[\"Honest\", \"Sybil\"])\n",
    "disp1.plot(ax=axes[0], values_format=\"d\", cmap=\"Blues\", colorbar=False)\n",
    "axes[0].set_title(f\"Local Model on {sybil_file}\")\n",
    "\n",
    "disp2 = ConfusionMatrixDisplay(confusion_matrix=global_cm, display_labels=[\"Honest\", \"Sybil\"])\n",
    "disp2.plot(ax=axes[1], values_format=\"d\", cmap=\"Blues\", colorbar=False)\n",
    "axes[1].set_title(f\"Global Model on {sybil_file}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"sybil_node_comparison.png\")\n",
    "print(\"[✓] Saved confusion matrix: sybil_node_comparison.png\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
